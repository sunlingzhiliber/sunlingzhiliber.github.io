---
layout:     post
title:      网站开发经历
date:       2018-07-10
author:     Liber Sun
header-img: img/post-basic.jpg
catalog: true
tags:
    - Web
    - RPC
    - Rest
    - 微服务
---

网站开发经历

# 应用发展过程

1.单一应用架构（Monoliths）

当网站流量很小的时候，我们只需要一个应用，就可以把所有的功能部署在一点，减少了部署的成本。此时，用于简化数据库CURD的数据访问框架，ORM(对象关系映射)是关键。

这样的代码存在这样的问题：每个功能严密耦合，无法满足高并发的业务需求。

2.垂直应用架构(纵向分解的思路)

随着我们系统业务的增多、访问量的增大，我们发现单机运行此系统已无法应付压力，因此我们将系统业务拆分为`互不关联的系统`，分别部署在各自的机器上，以划清逻辑并减少压力。但是系统与系统之间可以通过http 或 restful 的方式进行服务的调用。

![MVC系统](https://raw.githubusercontent.com/sunlingzhiliber/imgstore/master/20181126171349.png)

此时，在单个系统中用于加速开发的MVC框架是关键(分层的思想)。在单个系统中有各自的前端、后端和数据存储。在模块之间共享代码是严令禁止的。

3.分布式应用架构(RPC)

对于垂直子模块而言，它本身仍然属于一个完整的应用。
有一个单机应用所拥有的所有特点：web 层，服务层，持久层，
而且为了解决子应用与子应用之间的依赖，除了需要常规的 web 层外，通常还需要一层与 web 层同级的 api　层用于向依赖它的其他子应用提供服务，在功能上 web 层与 api 层最大的区别就是 web 包含返回页面，而 api 返回数据，两层之间有很多可以共用的逻辑，这就造成了代码的重复，
同时，由于 api 层对外提供服务，其他子应用由于业务需求需要新的服务时，提供服务的子应用不得不进行重新部署，即使它本应用的业务并为发生实质上的改变，这就造成了耦合。

![系统交互](https://raw.githubusercontent.com/sunlingzhiliber/imgstore/master/20181126190227.png)

为了解决这种情况，分布式服务架构便出现了，它将已经进行了垂直切分的应用再进行水平切分，将 web 层与服务层完全隔离开了，web 层单独成一个应用面向用户，服务层专职提供服务。

![水平切分](https://raw.githubusercontent.com/sunlingzhiliber/imgstore/master/20181126190335.png)

这种架构中，请求到达应用后，对请求的处理会被分布于多个模块中，然后每一模块产生的结果汇总成一个响应，发送回请求者。

此时，在不同模块之间的通讯联系手段，提高业务复用及整合的 RPC(Remote Procedure Call，远程过程调用) 是关键。
[RPC](#rpc原理)的目标是：使本程序调用其他远程主机上的函数，好像调用本程序内的函数一样简单，并且屏蔽了编程语言、操作系统的差异性。

RPC的调用有两种实现：
基于HTTP的RPC ，具有使用灵活，实现便捷（多种开源Web服务器支持），天生支持异构平台的调用。
基于TCP的RPC，效率更高，但实现起来更加复杂，由于协议不同和标准不同，难以跨平台。

4.流动计算架构(SOA)

(1) 当服务越来越多时，服务URL配置管理变得非常困难，F5硬件负载均衡器的单点压力也越来越大。

此时需要一个服务注册中心，动态的注册和发现服务，使服务的位置透明。

(2) 当进一步发展，服务间依赖关系变得错踪复杂，甚至分不清哪个应用要在哪个应用之前启动，架构师都不能完整的描述应用的架构关系。

这时，需要自动画出应用间的依赖关系图，以帮助架构师理清理关系。

(3) 接着，服务的调用量越来越大，服务的容量问题就暴露出来，这个服务需要多少机器支撑？什么时候该加机器？

为了解决这些问题，第一步，要将服务现在每天的调用量，响应时间，都统计出来，作为容量规划的参考指标。

其次，要可以动态调整权重，在线上，将某台机器的权重一直加大，并在加大的过程中记录响应时间的变化，直到响应时间到达阀值，记录此时的访问量，再以此访问量乘以机器数反推总容量。

这里的一系列都需要一个调度中心来对服务进行管控和监管。此时，用于提高机器利用率的 资源调度和治理中心(SOA) 是关键。

服务导向式架构（SOA）是集成多个较大组件（一般是应用）的一种机制，它们将整体构成一个彼此协作的套件。

这里简单的介绍一下dubbo：
![Dubbo架构图](https://raw.githubusercontent.com/sunlingzhiliber/imgstore/master/20181126173006.png)

Dubbo是一个阿里巴巴开源出来的一个分布式服务框架，致力于提供高性能和透明化的RPC远程服务调用方案，以及SOA服务治理方案。其核心部分包含:

远程通讯: 提供对多种基于长连接的NIO框架抽象封装，包括多种线程模型，序列化，以及“请求-响应”模式的信息交换方式。

集群容错: 提供基于接口方法的透明远程过程调用，包括多协议支持，以及软负载均衡，失败容错，地址路由，动态配置等集群支持。

自动发现: 基于注册中心目录服务，使服务消费方能动态的查找服务提供方，使地址透明，使服务提供方可以平滑增加或减少机器。

常见的使用模式是：Dubbo实现服务，Zookepper进行服务注册。

5.微服务

在微服务架构中，业务逻辑被拆分成一系列小而松散耦合的分布式组件，共同构成了较大的应用。每个组件都被称为微服务，而每个微服务都在整体架构中执行着单独的任务，或负责单独的功能。每个微服务可能会被一个或多个其他微服务调用，以执行较大应用需要完成的具体任务；

使用微服务架构还提供这样一种机制：增加新加入开发者的生产效率，并减少新功能的推广时长。每个微服务的代码库与相关工具集都很有限；开发者无需再去了解庞大而复杂的系统，只需理解自己所做的那部分微服务相关子集，便能贡献生产力。由于无需考虑应用的现有部分使用了什么语言或工具集，或者较大应用的其他开发者是否了解这些语言和工具，只需使用当前任务最趁手的工具，因此微服务开发起来非常迅速。

6.消息队列

主要原因是由于在高并发环境下，由于来不及同步处理，请求往往会发生堵塞，比如说，大量的insert，update之类的请求同时到达MySQL，直接导致无数的行锁表锁，甚至最后请求会堆积过多，从而触发too many connections错误。通过使用消息队列，我们可以异步处理请求，从而缓解系统的压力。

# RPC原理

由于各服务部署在不同机器，服务间的调用免不了网络通信过程，服务消费方每调用一个服务都要写一坨网络通信相关的代码，不仅复杂而且极易出错。

如果有一种方式能让我们像调用本地服务一样调用远程服务，而让调用者对网络通信这些细节透明，那么将大大提高生产力，比如服务消费方在执行service.sayHello("test")时，实质上调用的是远端的服务。这种方式其实就是RPC（Remote Procedure Call Protocol）

## 如何调用远程服务

要实现透明的网络通信细节，我们自然要对通信细节进行封装。

![调用流程](https://raw.githubusercontent.com/sunlingzhiliber/imgstore/master/20181126192508.png)

1）服务消费方（client）调用以本地调用方式调用服务；
2）client stub接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体；
3）client stub找到服务地址，并将消息发送到服务端；
4）server stub收到消息后进行解码；
5）server stub根据解码结果调用本地的服务；
6）本地服务执行并将结果返回给server stub；
7）server stub将返回结果打包成消息并发送至消费方；
8）client stub接收到消息，并进行解码；
9）服务消费方得到最终结果。

RPC的目标就是要2~8这些步骤都封装起来，让用户对这些细节透明。

## 如何对消息进行编码和解码

客户端和服务端相互通信的消息结构如何在网络中传输呢？我们考虑到了序列化的操作。为什么需要序列化？转换为二进制串后才好进行网络传输嘛！为什么需要反序列化？将二进制转换为对象才好进行后续处理！

现如今序列化的方案越来越多，每种序列化方案都有优点和缺点，它们在设计之初有自己独特的应用场景，那到底选择哪种呢？从RPC的角度上看，主要看三点：1）通用性，比如是否能支持Map等复杂的数据结构；2）性能，包括时间复杂度和空间复杂度，由于RPC框架将会被公司几乎所有服务使用，如果序列化上能节约一点时间，对整个公司的收益都将非常可观，同理如果序列化上能节约一点内存，网络带宽也能省下不少；3）可扩展性，对互联网公司而言，业务变化快，如果序列化协议具有良好的可扩展性，支持自动增加新的业务字段，删除老的字段，而不影响老的服务，这将大大提供系统的健壮性

## 通信

消息数据结构被序列化为二进制串后，下一步就要进行网络通信了。目前有两种IO通信模型：1）BIO；2）NIO。一般RPC框架需要支持这两种IO模型。

常见的是基于Netty-IO通信框架

## 如何发布自己的服务呢？

如何让别人使用我们的服务呢？有同学说很简单嘛，告诉使用者服务的IP以及端口就可以了啊。确实是这样，这里问题的关键在于是自动告知还是人肉告知。

有没有一种方法能实现自动告知，即机器的增添、剔除对调用方透明，调用者不再需要写死服务提供方地址？当然可以，现如今zookeeper被广泛用于实现服务自动注册与发现功能！

![zookeeper](https://raw.githubusercontent.com/sunlingzhiliber/imgstore/master/20181126192901.png)

# 微服务和SOA的区别

![区别](https://raw.githubusercontent.com/sunlingzhiliber/imgstore/master/20181126210718.png)

`SOA`可以看成一个大容器，在这个大容器里面，一个应用（application）的所有组件（components）紧密组装在一起。

`微服务`则是把一个应用（application）进一步拆分成了多个小型自治的服务（services）集合

---------------------

我们这里以购物网站为例子：

使用SOA，我们会将购物网站拆分成主要的业务逻辑组，并将每个部分作为独立的应用开发，最后集成到一起。

使用微服务，会将购物网站切分成较小的任务导向服务。他不在是一个购物网站的应用，而各种服务。

![比较](https://raw.githubusercontent.com/sunlingzhiliber/imgstore/master/20181126210655.png)

微服务与SOA有很多相同之处。两者都属于典型的、包含松耦合分布式组件的系统结构。但是两种架构背后的意图是不同的：SOA尝试将应用集成，一般采用中央管理模式来确保各应用能够交互运作。微服务尝试部署新功能，快速有效地扩展开发团队。它着重于分散管理、代码再利用与自动化执行。

| 架构     | SOA              | 微服务                   |
|----------|------------------|--------------------------|
| 组建大小 | 大块业务逻辑     | 单独任务或小块业务逻辑   |
| 耦合     | 通常松耦合       | 总是松耦合               |
| 公司架构 | 大型公司         | 小型、专注功能交叉的团队 |
| 管理     | 中央管理         | 分散管理                 |
| 目标     | 应用能够交互操作 | 快速扩展开发团队         |

# 互联网架构的基础原则

- 尽可能拆分
- 服务架构“去中心化”
- 异步化
- 数据化运营
- 尽可能使用成熟组件
- 尽可能自动化

“尽可能拆分”，是以服务化的方式去结构我们的应用系统，要做到更好的独立扩展与伸缩，更灵活的部署和检查错误。

“去中心化”，其目的在于减少故障影响面。

“异步化”，利用异步机制拆分事务，系统解耦，提高开发效率。这在一定程度上会加大系统的复杂程度，我们需要确保系统最终一致。

“数据化运营”，在于我们要真正的发掘数据的价值，而不仅仅是数据的提供平台。我们要根据数据定位业务业务，从而更好的监控系统。

“自动化”，运维是一个坑啊。包括运维标准规范和平台化、弹性伸缩自动化、部署自动化、故障处理自动化。

# 淘宝服务端分布式架构演进之路

参考[爪哇笔记](https://mp.weixin.qq.com/s/qsk0Wv8eg_ygpy4NC4aUkA)

## 1.概述

一个应用的发展总是不断演进的过程，以淘宝为例子，介绍从一百个并发到千万级并发情况下服务端的架构的演进过程，同时列举出每个演进阶段会遇到的相关技术，让大家对架构的演进有一个整体的认知，最后汇总了一些架构设计的原则。

## 2.基本概念

- **分布式**
  系统中的多个模块在不同服务器上部署，即可称为分布式系统，如Tomcat和数据库分别部署在不同的服务器上，或两个相同功能的Tomcat分别部署在不同服务器上
- **高可用**
  系统中部分节点失效时，其他节点能够接替它继续提供服务，则可认为系统具有高可用性
- **集群**
  一个特定领域的软件部署在多台服务器上并作为一个整体提供一类服务，这个整体称为集群。如Zookeeper中的Master和Slave分别部署在多台服务器上，共同组成一个整体提供集中配置服务。在常见的集群中，客户端往往能够连接任意一个节点获得服务，并且当集群中一个节点掉线时，其他节点往往能够自动的接替它继续提供服务，这时候说明集群具有高可用性
- **负载均衡**
  请求发送到系统时，通过某些方式把请求均匀分发到多个节点上，使系统中每个节点能够均匀的处理请求负载，则可认为系统是负载均衡的
- **正向**、**反向代理**
  系统内部要访问外部网络时，统一通过一个代理服务器把请求转发出去，在外部网络看来就是代理服务器发起的访问，此时代理服务器实现的是正向代理；当外部请求进入系统时，代理服务器把该请求转发到系统中的某台服务器上，对外部请求来说，与之交互的只有代理服务器，此时代理服务器实现的是反向代理。简单来说，正向代理是代理服务器代替系统内部来访问外部网络的过程，反向代理是外部请求访问系统时通过代理服务器转发到内部服务器的过程。

## 3.演进

### 3.1 单机架构

![单机架构](https://raw.githubusercontent.com/sunlingzhiliber/imgstore/master/20190604170828.png)

在网站的初期，应用数量以及用户数都比较少，我们可以将Tomcat和数据库部署在同一台服务器，然后将IP和www.taobao.com进行绑定。

### 3.2 第一次演进：Tomcat与数据库分开部署

![分开部署](https://raw.githubusercontent.com/sunlingzhiliber/imgstore/master/20190604170905.png)


Tomcat和数据库分别独占服务器资源，显著提高两者各自性能。

### 3.3  第二次演进：引入本地缓存和分布式缓存

![缓存](https://raw.githubusercontent.com/sunlingzhiliber/imgstore/master/20190604170923.png)

在Tomcat同服务器上或同JVM中增加本地缓存，并在外部增加分布式缓存，缓存热门商品信息或热门商品的html页面等。通过缓存能把绝大多数请求在读写数据库前拦截掉，大大降低数据库压力。其中涉及的技术包括：使用memcached作为本地缓存，使用Redis作为分布式缓存，还会涉及缓存一致性、缓存穿透/击穿、缓存雪崩、热点数据集中失效等问题。

### 3.4 第三次演进：引入反向代理实现负载均衡

![负载均衡](https://raw.githubusercontent.com/sunlingzhiliber/imgstore/master/20190604170944.png)

在多台服务器上分别部署Tomcat，使用反向代理软件（Nginx）把请求均匀分发到每个Tomcat中。此处假设Tomcat最多支持100个并发，Nginx最多支持50000个并发，那么理论上Nginx把请求分发到500个Tomcat上，就能抗住50000个并发。其中涉及的技术包括：Nginx、HAProxy，两者都是工作在网络第七层的反向代理软件，主要支持http协议，还会涉及session共享、文件上传下载的问题。

**反向代理使应用服务器可支持的并发量大大增加，但并发量的增长也意味着更多请求穿透到数据库，单机的数据库最终成为瓶颈**

### 3.5 第四次演进：数据库读写分离

![读写分离](https://raw.githubusercontent.com/sunlingzhiliber/imgstore/master/20190604171001.png)

把数据库划分为读库和写库，读库可以有多个，通过同步机制把写库的数据同步到读库，对于需要查询最新写入数据场景，可通过在缓存中多写一份，通过缓存获得最新数据。其中涉及的技术包括：Mycat，它是数据库中间件，可通过它来组织数据库的分离读写和分库分表，客户端通过它来访问下层数据库，还会涉及数据同步，数据一致性的问题。

**业务逐渐变多，不同业务之间的访问量差距较大，不同业务直接竞争数据库，相互影响性能**

### 3.6 第五次演进：数据库按业务分库

![业务分库](https://raw.githubusercontent.com/sunlingzhiliber/imgstore/master/20190604171036.png)

把不同业务的数据保存到不同的数据库中，使业务之间的资源竞争降低，对于访问量大的业务，可以部署更多的服务器来支撑。这样同时导致跨业务的表无法直接做关联分析，需要通过其他途径来解决

**随着用户数的增长，单机的写库会逐渐会达到性能瓶颈**

### 3.7 第六次演进：把大表拆分为小表

![拆表](https://raw.githubusercontent.com/sunlingzhiliber/imgstore/master/20190604171051.png)

比如针对评论数据，可按照商品ID进行hash，路由到对应的表中存储；针对支付记录，可按照小时创建表，每个小时表继续拆分为小表，使用用户ID或记录编号来路由数据。只要实时操作的表数据量足够小，请求能够足够均匀的分发到多台服务器上的小表，那数据库就能通过水平扩展的方式来提高性能。其中前面提到的Mycat也支持在大表拆分为小表情况下的访问控制。

这种做法显著的增加了数据库运维的难度，对DBA的要求较高。数据库设计到这种结构时，已经可以称为分布式数据库，但是这只是一个逻辑的数据库整体，数据库里不同的组成部分是由不同的组件单独来实现的，如分库分表的管理和请求分发，由Mycat实现，SQL的解析由单机的数据库实现，读写分离可能由网关和消息队列来实现，查询结果的汇总可能由数据库接口层来实现等等，这种架构其实是MPP（大规模并行处理）架构的一类实现。

目前开源和商用都已经有不少MPP数据库，开源中比较流行的有Greenplum、TiDB、Postgresql XC、HAWQ等，商用的如南大通用的GBase、睿帆科技的雪球DB、华为的LibrA等等，不同的MPP数据库的侧重点也不一样，如TiDB更侧重于分布式OLTP场景，Greenplum更侧重于分布式OLAP场景，这些MPP数据库基本都提供了类似Postgresql、Oracle、MySQL那样的SQL标准支持能力，能把一个查询解析为分布式的执行计划分发到每台机器上并行执行，最终由数据库本身汇总数据进行返回，也提供了诸如权限管理、分库分表、事务、数据副本等能力，并且大多能够支持100个节点以上的集群，大大降低了数据库运维的成本，并且使数据库也能够实现水平扩展。

**数据库和Tomcat都能够水平扩展，可支撑的并发大幅提高，随着用户数的增长，最终单机的Nginx会成为瓶颈**

### 3.8 第七次演进：使用LVS或F5来使多个Nginx负载均衡

![LVS或F5来使多个Nginx负载均衡](https://raw.githubusercontent.com/sunlingzhiliber/imgstore/master/20190604171122.png)

由于瓶颈在Nginx，因此无法通过两层的Nginx来实现多个Nginx的负载均衡。图中的LVS和F5是工作在网络第四层的负载均衡解决方案，其中LVS是软件，运行在操作系统内核态，可对TCP请求或更高层级的网络协议进行转发，因此支持的协议更丰富，并且性能也远高于Nginx，可假设单机的LVS可支持几十万个并发的请求转发；F5是一种负载均衡硬件，与LVS提供的能力类似，性能比LVS更高，但价格昂贵。由于LVS是单机版的软件，若LVS所在服务器宕机则会导致整个后端系统都无法访问，因此需要有备用节点。可使用keepalived软件模拟出虚拟IP，然后把虚拟IP绑定到多台LVS服务器上，浏览器访问虚拟IP时，会被路由器重定向到真实的LVS服务器，当主LVS服务器宕机时，keepalived软件会自动更新路由器中的路由表，把虚拟IP重定向到另外一台正常的LVS服务器，从而达到LVS服务器高可用的效果。

此处需要注意的是，上图中从Nginx层到Tomcat层这样画并不代表全部Nginx都转发请求到全部的Tomcat，在实际使用时，可能会是几个Nginx下面接一部分的Tomcat，这些Nginx之间通过keepalived实现高可用，其他的Nginx接另外的Tomcat，这样可接入的Tomcat数量就能成倍的增加。

**由于LVS也是单机的，随着并发数增长到几十万时，LVS服务器最终会达到瓶颈，此时用户数达到千万甚至上亿级别，用户分布在不同的地区，与服务器机房距离不同，导致了访问的延迟会明显不同。**

### 3.9 第八次演进：通过DNS轮询实现机房间的负载均衡

![DNS轮询](https://raw.githubusercontent.com/sunlingzhiliber/imgstore/master/20190604171147.png)

在DNS服务器中可配置一个域名对应多个IP地址，每个IP地址对应到不同的机房里的虚拟IP。当用户访问www.taobao.com时，DNS服务器会使用轮询策略或其他策略，来选择某个IP供用户访问。此方式能实现机房间的负载均衡，至此，系统可做到机房级别的水平扩展，千万级到亿级的并发量都可通过增加机房来解决，系统入口处的请求并发量不再是问题。

**随着数据的丰富程度和业务的发展，检索、分析等需求越来越丰富，单单依靠数据库无法解决如此丰富的需求**

### 3.10 第九次演进：引入NoSQL数据库和搜索引擎等技术

![](https://raw.githubusercontent.com/sunlingzhiliber/imgstore/master/20190604171213.png)

当数据库中的数据多到一定规模时，数据库就不适用于复杂的查询了，往往只能满足普通查询的场景。对于统计报表场景，在数据量大时不一定能跑出结果，而且在跑复杂查询时会导致其他查询变慢，对于全文检索、可变数据结构等场景，数据库天生不适用。因此需要针对特定的场景，引入合适的解决方案。如对于海量文件存储，可通过分布式文件系统HDFS解决，对于key value类型的数据，可通过HBase和Redis等方案解决，对于全文检索场景，可通过搜索引擎如ElasticSearch解决，对于多维分析场景，可通过Kylin或Druid等方案解决。

当然，引入更多组件同时会提高系统的复杂度，不同的组件保存的数据需要同步，需要考虑一致性的问题，需要有更多的运维手段来管理这些组件等。

**引入更多组件解决了丰富的需求，业务维度能够极大扩充，随之而来的是一个应用中包含了太多的业务代码，业务的升级迭代变得困难**

### 3.11 第十次演进：大应用拆分为小应用

![](https://raw.githubusercontent.com/sunlingzhiliber/imgstore/master/20190604171226.png)

按照业务板块来划分应用代码，使单个应用的职责更清晰，相互之间可以做到独立升级迭代。这时候应用之间可能会涉及到一些公共配置，可以通过分布式配置中心Zookeeper来解决。

**不同应用之间存在共用的模块，由应用单独管理会导致相同代码存在多份，导致公共功能升级时全部应用代码都要跟着升级**

### 3.12 第十一次演进：复用的功能抽离成微服务

![](https://raw.githubusercontent.com/sunlingzhiliber/imgstore/master/20190604171235.png)

如用户管理、订单、支付、鉴权等功能在多个应用中都存在，那么可以把这些功能的代码单独抽取出来形成一个单独的服务来管理，这样的服务就是所谓的微服务，应用和服务之间通过HTTP、TCP或RPC请求等多种方式来访问公共服务，每个单独的服务都可以由单独的团队来管理。此外，可以通过Dubbo、SpringCloud等框架实现服务治理、限流、熔断、降级等功能，提高服务的稳定性和可用性。

**不同服务的接口访问方式不同，应用代码需要适配多种访问方式才能使用服务，此外，应用访问服务，服务之间也可能相互访问，调用链将会变得非常复杂，逻辑变得混乱**

### 3.13 第十二次演进：引入企业服务总线ESB屏蔽服务接口的访问差异

![](https://raw.githubusercontent.com/sunlingzhiliber/imgstore/master/20190604171247.png)

通过ESB统一进行访问协议转换，应用统一通过ESB来访问后端服务，服务与服务之间也通过ESB来相互调用，以此降低系统的耦合程度。这种单个应用拆分为多个应用，公共服务单独抽取出来来管理，并使用企业消息总线来解除服务之间耦合问题的架构，就是所谓的SOA（面向服务）架构，这种架构与微服务架构容易混淆，因为表现形式十分相似。个人理解，微服务架构更多是指把系统里的公共服务抽取出来单独运维管理的思想，而SOA架构则是指一种拆分服务并使服务接口访问变得统一的架构思想，SOA架构中包含了微服务的思想。

**业务不断发展，应用和服务都会不断变多，应用和服务的部署变得复杂，同一台服务器上部署多个服务还要解决运行环境冲突的问题，此外，对于如大促这类需要动态扩缩容的场景，需要水平扩展服务的性能，就需要在新增的服务上准备运行环境，部署服务等，运维将变得十分困难**

### 3.14 第十三次演进：引入容器化技术实现运行环境隔离与动态服务管理

![](https://raw.githubusercontent.com/sunlingzhiliber/imgstore/master/20190604171302.png)

目前最流行的容器化技术是Docker，最流行的容器管理服务是Kubernetes(K8S)，应用/服务可以打包为Docker镜像，通过K8S来动态分发和部署镜像。Docker镜像可理解为一个能运行你的应用/服务的最小的操作系统，里面放着应用/服务的运行代码，运行环境根据实际的需要设置好。把整个“操作系统”打包为一个镜像后，就可以分发到需要部署相关服务的机器上，直接启动Docker镜像就可以把服务起起来，使服务的部署和运维变得简单。

在大促的之前，可以在现有的机器集群上划分出服务器来启动Docker镜像，增强服务的性能，大促过后就可以关闭镜像，对机器上的其他服务不造成影响（在3.14节之前，服务运行在新增机器上需要修改系统配置来适配服务，这会导致机器上其他服务需要的运行环境被破坏）。

**使用容器化技术后服务动态扩缩容问题得以解决，但是机器还是需要公司自身来管理，在非大促的时候，还是需要闲置着大量的机器资源来应对大促，机器自身成本和运维成本都极高，资源利用率低**

### 3.15 第十四次演进：以云平台承载系统

![](https://raw.githubusercontent.com/sunlingzhiliber/imgstore/master/20190604171312.png)

系统可部署到公有云上，利用公有云的海量机器资源，解决动态硬件资源的问题，在大促的时间段里，在云平台中临时申请更多的资源，结合Docker和K8S来快速部署服务，在大促结束后释放资源，真正做到按需付费，资源利用率大大提高，同时大大降低了运维成本。

所谓的云平台，就是把海量机器资源，通过统一的资源管理，抽象为一个资源整体，在之上可按需动态申请硬件资源（如CPU、内存、网络等），并且之上提供通用的操作系统，提供常用的技术组件（如Hadoop技术栈，MPP数据库等）供用户使用，甚至提供开发好的应用，用户不需要关系应用内部使用了什么技术，就能够解决需求（如音视频转码服务、邮件服务、个人博客等）。在云平台中会涉及如下几个概念：
- IaaS：基础设施即服务。对应于上面所说的机器资源统一为资源整体，可动态申请硬件资源的层面；
- PaaS：平台即服务。对应于上面所说的提供常用的技术组件方便系统的开发和维护；
- SaaS：软件即服务。对应于上面所说的提供开发好的应用或服务，按功能或性能要求付费。
  
**至此，以上所提到的从高并发访问问题，到服务的架构和系统实施的层面都有了各自的解决方案，但同时也应该意识到，在上面的介绍中，其实是有意忽略了诸如跨机房数据同步、分布式事务实现等等的实际问题，这些问题以后有机会再拿出来单独讨论**


### 4.总结

**架构的调整**并不是说要严格遵循以上的演变路径，在实现状况中，要根据系统已存在的几个问题，或者可能最先达到瓶颈的问题为突破可，进行解决。

**架构的设计**是不是就要一步到位？对于单次实施并且性能指标明确的系统，架构设计到能够支持系统的性能指标要求就足够了，但要留有扩展架构的接口以便不备之需。对于不断发展的系统，如电商平台，应设计到能满足下一阶段用户量和性能指标要求的程度，并根据业务的增长不断的迭代升级架构，以支持更高的并发和更丰富的业务。