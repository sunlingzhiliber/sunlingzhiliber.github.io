---
layout:     post
title:      分布式
author:     Liber Sun
header-img: img/post-basic.jpg
catalog: true
tags:
---

# 分布式

我们这里首先需要区分分布式和集群的区别。

>计算机`集群`简称集群是一种计算机系统，它通过一组松散集成的计算机软件和/或硬件连接起来高度紧密地协作完成计算工作。在某种意义上，他们可以被看作是一台计算机。集群系统中的单个计算机通常称为节点，通常通过局域网连接，但也有其它的可能连接方式。集群计算机通常用来改进单个计算机的计算速度和/或可靠性。一般情况下集群计算机比单个计算机，比如工作站或超级计算机性能价格比要高得多。

集群就是同一个代码在不同的计算机上运行。

>分布式系统是一组计算机，通过网络相互连接传递消息与通信后并协调它们的行为而形成的系统。组件之间彼此进行交互以实现一个共同的目标。

分布式是将应用程序按模块功能进行划分，模块之间独立部署到不同的服务器，在使用时通过多个独立模块的相互调用从而构成一个应用。

# 分布式和服务化模式

什么是"分布式系统"？如果一下子就想到微服务、注册中心，就意味着你把服务化的模式(SOA,微服务)和分布式系统错误的划上了等号。

什么是服务化？服务化的`本质`是`分而治之`，首先我们需要进行拆分，然后才能进行治理。在拆分的过程中，高内聚，低耦合的思想就起了一个非常重要的作用，我们要求
拆分之后的各个部分之间协作的复杂度应该在一个比较低的程度。而且往往拆分并不是一次性的拆分，往往需要在不断的实践中，循序渐进的拆分。
而我们采用了何种服务模式(SOA、微服务等)，并不重要。

可以看到服务化体现了“分治”的效果，这也是分布式系统的核心思想，因此从“分治”这个本质上来看，服务化的确是分布式系统，但分布式系统不仅仅停留在那些服务化的模式上。

我们在解决一个问题的时候，需要做到拆分，通过梳理、归类，将不同的紧密相关的部分收敛到一个独立的逻辑体中。

## 分布式服务

分布式服务是一个挺有意思的东西，也很常用，简单来说，就是把服务组件部署在不同节点上，通过rpc的方式访问，为了实现这一功能，我们需要考虑通信协议，序列化方式，进一步来说，我们还要了解如何做服务注册和发现，以及如何做限流，做服务熔断和降级，等等等等。

常见的分布式服务框架有dubbo，以及Spring Cloud这类产品，学会使用他们，然后了解它们的底层实现原理，相信会是一个很有趣的过程。


# 分布式和中间件

又或许一提到"分布式系统"，我们就想到了MQ(消息队列)框架，RPC(Remote Procedure Call)框架或者DAL(Distributed Data Access Layer)框架，这样我们就把中间件和分布式系统划上了等号。

这里我们要认识到，中间件起了一个什么样的作用？标准化的作用!!!
中间件承载了标准化思想，在一定成都上起到了引导和约束的作用，以此减少了系统的复杂成都和协作成本。

- MQ框架标准化了不同应用程序间`非实时异步通信`的方式。
- RPC框架标准化了不同应用程序间实时通讯的方式。
- DAL框架标准化了应用程序和数据库之间通讯的方式。

为什么我们需要标准化？这也是中间件存在的一个价值：避免我们将精力过多的花费在某个子功能下众多差异不大的选项中。
以DAL为例，标准化了访问数据库的过程；据库访问框架的作用是为了标准化操作不同数据库的差异，使得上层应用程序不用纠结于该怎么与mysql交互或者该怎么与SQL SERVER交互。

分布式系统必定会使用中间件，但分布式系统绝不仅仅是用了什么中间件。要了解每个中间件做了什么，以及它带来的优点和缺点，这样你才能从不同的，日新月异的框架中选出真正适合当前系统的技术框架。


# 分布式理论

## CAP

任何基于网络的数据共享系统最多只能满足数据一致性(Consistency)、可用性(Availability)和网络分区容忍(Partition Tolerance)三个特性中的两个，三个特性的定义如下：

- 数据一致性：等同于所有节点拥有数据的最新版本
- 可用性：数据具备高可用性
- 分区容忍：容忍网络出现分区，分区之间网络不可达

在大规模的分布式网络下，分区容忍是必须接受的现实。因此我们只能在一致性和可用性中做出选择。CAP似乎对于分布式系统给出了一个悲观的结局。但是我们要认识到

>CAP理论是对分布式系统中一个数据无法同时达到可用性和一致性的断言，而一个系统中往往存在很多类型的数据，部分数据（譬如银行账户中的余额）是需要强一致性的，而另外一部分数据（譬如银行的总客户数）并不要求强一致性，所以拿CAP理论来划分整个系统是不严谨的， CAP理论带来的价值是指引我们在设计分布式系统时需要区分各种数据的特点，并仔细考虑在小概率的网络分区发生时究竟为该数据选择可用性还是一致性。

-------------------------------------------

>对CAP理论的另外一种误读是系统设计时选择其一而完全不去优化另外一项，可用性和一致性的取值范围并不是只有0和1，可用性的值域可以定义成0到100%的连续区间，而一致性也可分为强一致性、弱一致性、读写一致性、最终一致性等多个不同的强弱等级，细想下去CAP理论定义的其实是在容忍网络分区的条件下，“强一致性”和“极致可用性”无法同时达到。


当你使用分布式架构的时候，你必然会碰到CAP的问题。我们来看看CAP意味着什么？

C：数据一致性(consistency)
所有节点拥有数据的最新版本
A：可用性(availability)
数据具备高可用性
P：分区容错性(partition-tolerance)
容忍网络出现分区，分区之间网络不可达。

这三点中我们要意识到，分区容错性是必须的，是客观世界一定存在的问题。

但是我们也需要认识到，不是说为了P,我们就只能在 C 和 A 中选择一个。
分区虽然是一定存在的，但是出现的情况是比较少的，这样 CAP 就在大多数的时间上能够满足 C 和 A。

## BASE

BASE理论是由eBay架构师提出的，是基本可用（Basically Available）、软状态（Soft State）和最终一致性（Eventually Consistent）三个短语的缩写。

BASE是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网分布式系统实践的总结，是基于CAP定律逐步演化而来。

>其核心思想是即使无法做到强一致性，但每个应用都可以根据自身业务特点，才用适当的方式来使系统打到最终一致性。

BASE理论满足三点内容：

- 基本可用
  什么是基本可用？假设系统出现了问题，但是相较于正常的系统而言，还是可以使用的。
- 软状态
  相对于原子性而言，要求多个节点的数据副本都是一致的，这是一种“硬状态”。允许系统中的数据存在中间状态，并认为该状态不影响系统的整体可用性，即允许系统在多个不同节点的数据副本存在数据延时。
- 最终一致性
  上面说软状态，然后不可能一直是软状态，必须有个时间期限。在期限过后，应当保证所有副本保持数据一致性，从而达到数据的最终一致性。

总体来说BASE理论面向的是大型高可用、可扩展的分布式系统。BASE提出通过牺牲强一致性来获得可用性，并允许数据段时间内的不一致，但是最终达到一致状态。

由于BASE理论需要在一致性和可用性方面做出权衡，因此涌现了很多关于一致性的算法和协议。其中比较著名的有二阶提交协议（2 Phase Commitment Protocol），三阶提交协议（3 Phase Commitment Protocol）和Paxos算法。


## 2PC

二阶段提交的算法思路可以概括为：每个参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情报，决定各参与者是否要提交操作还是中止操作。

![协调者与参与者](https://user-gold-cdn.xitu.io/2018/6/17/1640df7a75e16c15?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

所谓的两个阶段分别是：

第一阶段：准备阶段（投票阶段）
第二阶段：提交阶段（执行阶段）

### 准备阶段

准备阶段分为三个步骤：

![准备阶段](https://user-gold-cdn.xitu.io/2018/6/17/1640df7a75fcc98e?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

a.事务询问。

>协调者向所有的参与者询问，是否准备好了执行事务，并开始等待各参与者的响应。

b.执行事务

>各参与者节点执行事务操作。如果本地事务成功，将Undo和Redo信息记入事务日志中，但不提交；否则，直接返回失败，退出执行。

c.各参与者向协调者反馈事务询问的响应

>如果参与者成功执行了事务操作，那么就反馈给协调者 Yes响应，表示事务可以执行提交；如果参与者没有成功执行事务，就返回No给协调者，表示事务不可以执行提交。

### 提交阶段

在提交阶段中，会根据准备阶段的投票结果执行2种操作：执行事务提交，中断事务。

提交事务过程：

![提交事务过程：](https://user-gold-cdn.xitu.io/2018/6/17/1640df7a75f938f2?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

a. 发送提交请求

>协调者向所有参与者发出commit请求。

b. 事务提交

>参与者收到commit请求后，会正式执行事务提交操作，并在完成提交之后，释放整个事务执行期间占用的事务资源。

c. 反馈事务提交结果

>参与者在完成事务提交之后，向协调者发送Ack信息。

d. 事务提交确认

>协调者接收到所有参与者反馈的Ack信息后，完成事务。

中断事务过程

![中断事务过程](https://user-gold-cdn.xitu.io/2018/6/17/1640df7a760851c6?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

a. 发送回滚请求

>协调者向所有参与者发出Rollback请求。

b. 事务回滚

>参与者接收到Rollback请求后，会利用其在提交阶段种记录的Undo信息，来执行事务回滚操作。在完成回滚之后，释放在整个事务执行期间占用的资源。

c. 反馈事务回滚结果

>参与者在完成事务回滚之后，想协调者发送Ack信息。

d. 事务中断确认

>协调者接收到所有参与者反馈的Ack信息后，完成事务中断。


### 优缺点

优点：原理简单，实现方便。
缺点：同步阻塞，单点问题，数据不一致，容错性不好。

1.同步阻塞
在二阶段提交的过程中，所有的节点都在等待其他节点的响应，无法进行其他操作。这种同步阻塞极大的限制了分布式系统的性能。
2.单点问题
协调者在整个二阶段提交过程中很重要，如果协调者在提交阶段出现问题，那么整个流程将无法运转。更重要的是，其他参与者将会处于一直锁定事务资源的状态中，而无法继续完成事务操作。
3.数据不一致
假设当协调者向所有的参与者发送commit请求之后，发生了局部网络异常，或者是协调者在尚未发送完所有 commit请求之前自身发生了崩溃，导致最终只有部分参与者收到了commit请求。这将导致严重的数据不一致问题。
4.容错性不好
如果在二阶段提交的提交询问阶段中，参与者出现故障，导致协调者始终无法获取到所有参与者的确认信息，这时协调者只能依靠其自身的超时机制，判断是否需要中断事务。显然，这种策略过于保守。换句话说，二阶段提交协议没有设计较为完善的容错机制，任意一个节点是失败都会导致整个事务的失败。



## 3PC

![3Pc](https://user-gold-cdn.xitu.io/2018/6/17/1640df8c8350c673?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

与两阶段提交不同的是，三阶段提交有两个改动点。

1. 引入超时机制 - 同时在协调者和参与者中都引入超时机制。
2. 在第一阶段和第二阶段中插入一个准备阶段，保证了在最后提交阶段之前各参与节点的状态是一致的。

相对于二阶段提交，三阶段提交主要解决的单点故障问题，并减少了阻塞的时间。
因为一旦参与者无法及时收到来自协调者的信息之后，他会默认执行 commit。而不会一直持有事务资源并处于阻塞状态。

三阶段提交也会导致数据一致性问题。由于网络原因，协调者发送的 abort 响应没有及时被参与者接收到，那么参与者在等待超时之后执行了 commit 操作。
这样就和其他接到 abort 命令并执行回滚的参与者之间存在数据不一致的情况。

## Paxos

无法达到同时达到强一致性和极致可用性，但我们可以根据数据类型在二者中选择其一后去优化另外一个，Paxos协议就是一种在保证强一致性前提下把可用性优化到极限的算法。

Paxos 算法是 基于消息传递 且具有 高效容错特性 的一致性算法，目前公认的解决 分布式一致性问题 最有效的算法之一.

只要系统中2f+1个节点中的f+1个节点可用，那么系统整体就可用并且能保证数据的强一致性，它对于可用性的提升是极大的。

具体步骤如下：

1. Proposer选择一个提案编号n，然后向半数以上的Acceptors发送编号为n的prepare请求。
2. 如果一个Acceptor收到一个编号为n 的prepare请求，且 n 大于它已经响应的所有prepare请求的编号，那么它就会保证不会再通过(accept)任何编号小于 n 的提案，同时将它已经通过的最大编号的提案(如果存在的话)作为响应。

3. 如果Proposer收到来自半数以上的Acceptor对于它的prepare请求(编号为n )的响应，那么它就会发送一个针对编号为 n ，value值为 v 的提案的accept请求给Acceptors，在这里 v 是收到的响应中编号最大的提案的值，如果响应中不包含提案，那么它就是任意值。
4. 如果Acceptor收到一个针对编号n 的提案的accept请求，只要它还未对编号大于 n 的prepare请求作出响应，它就可以通过这个提案。

## ZAB

Paxos协议虽然是完备的，但要把它应用到实际的分布式系统中还有些问题要解决：

- 在多个Proposer的场景下，Paxos不保证先提交的提案先被接受，实际应用中要保证多提案被接受的先后顺序怎么办？
- Paxos允许多个Proposer提交提案，那有可能出现活锁问题，出现场景是这样的：提案n在第二阶段还没有完成时，新的提案n+1的第一阶段prepare请求到达Acceptor，按协议规定Acceptor将响应新提案的prepare请求并保证不会接受小于n+1的任何请求，这可能导致提案n将不会被通过，同样在n+1提案未完成第二阶段时，假如提案n的提交者又提交了n+2提案，这可能导致n+1提案也无法通过。
- Paxos协议规定提案的值v只要被大多数Acceptor接受过，后续的所有提案不能修改值v，那现实情况下我还要修改v值怎么办？

ZooKeeper的核心算法ZAB通过一个简单的约束解决了前2个问题：所有提案都转发到唯一的Leader（通过Leader选举算法从Acceptor中选出来的）来提交，由Leader来保证多个提案之间的先后顺序，同时也避免了多Proposer引发的活锁问题。

ZAB引入Leader后也会带来一个新问题： Leader宕机了怎么办？其解决方案是选举出一个新的Leader。

从保证一致性的算法核心角度看ZAB确实是借鉴了Paxos的多数派思想，但它提供的全局时序保证以及ZooKeeper提供给用户可修改的znode才让Paxos在开源界大放异彩，所以ZAB的价值不仅仅是提供了Paxos算法的优化实现，也难怪ZAB的作者一直强调ZAB和Paxos是不一样的算法。

## Raft

Raft 也是一个 一致性算法，和 Paxos 目标相同。但它还有另一个名字 - 易于理解的一致性算法。Paxos 和 Raft 都是为了实现 一致性 产生的。这个过程如同选举一样，参选者 需要说服 大多数选民 (服务器) 投票给他，一旦选定后就跟随其操作。Paxos 和 Raft 的区别在于选举的 具体过程 不同。

## CAP、Paxos、ZAB

CAP理论告诉我们在分布式环境下网络分区无法避免，需要去权衡选择数据的一致性和可用性，Paxos协议提出了一种极其简单的算法在保障数据一致性时最大限度的优化了可用性，ZooKeeper的ZAB协议把Paxos更加简化，并提供全局时序保证，使得Paxos能够广泛应用到工业场景。










# 分布式***

## 分布式事务

指事务的操作位于不同的节点上，需要保证事务的 ACID 特性。

例如在下单场景下，库存和订单如果不在同一个节点上，就涉及分布式事务。

分布式事务是一个复杂的概念，主要指分布式系统中需要强一致场景时所用到的事务。理解和实现它都不是简单的事情。

如果我们退而求其次，不要求强一致性，而选择最终一致性，则可以用更加灵活的方案，比如事务消息。

## 分布式Session

分布式session一般有多种实现方式，可以存数据库或者缓存，也可以单独部署成一个服务，总之最重要的一点就是，性能要好，并且要高可用。

## 分布式锁

在单机场景下，可以使用语言的内置锁来实现进程同步。但是在分布式场景下，需要同步的进程可能位于不同的节点上，那么就需要使用分布式锁。

分布式锁则用于一些需要一致性的场景中，比如订单生成这种全局唯一的功能，分布式锁通常可以用缓存或者数据库来实现，但为了保证高性能，并且避免死锁，我们一般采用Redis或者zookeeper来实现。

# 分布式技术

## RPC

RPC（Remote Procedure Call）—远程过程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。
比如两个不同的服务A,B部署在两台不同的机器上，那么服务 A 如果想要调用服务 B 中的某个方法该怎么办呢？使用 HTTP请求 当然可以，但是可能会比较慢而且一些优化做的并不好。RPC 的出现就是为了解决这个问题。

使用RPC，能让我们像调用本地服务一样调用远程服务，而让调用者对网络通信这些细节透明，那么将大大提高生产力，比如服务消费方在执行orderService.buy("HHKB键盘")时，实质上调用的是远端的服务。

在RPC框架中，存在三个主要角色，分别是Provider、Consumer以及Registry。

实现RPC我们需要用到的技术涉及到：

- 动态代理,生成client和server。
- 序列化，传输java对象需要序列化
- NIO，基于Netty的IO通信框架
- 服务注册中心 ，Redis、Zookeeper、Consul、Etcd。










# 分布式算法

## 互斥算法

当有多个进程竞争系统中相同的资源时，互斥问题就产生了。一个正确的互斥算法必须避免冲突( 死锁和饿死) 和保证公平性。通常分布式操作系统使用的互斥算法有:集中式算法、分布式算法和令牌环算法。

### 集中式算法

集中式算法借鉴了集中式互斥算法的思想，在分布式系统中，选出一个进程为协调者 (通过科学的分析制定一套规则) 。协调者对所有的请求进行排队并根据一定的规则授予许可。协调者接受请求以后，检查临界区内的资源是否被其他进程占用。如果是，则它将当前请求进程插入到对应临界资源的请求队列中; 否则，回复一个同意消息给请求进程，通知它可以访问该临界资源。
该算法通俗易懂，既能够杜绝死锁、饥饿等现象发生，又能保证资源的互斥访问顺利进行。但是它也有缺点，由于是集中式管理，所以一旦管理进程出现故障，则整个系统将处于瘫痪状态。因此，管理进程的性能完全决定了算法的效率，应用范围小，难以普及。

### 分布式算法

分布式算法中运用到广播请求通信，当进程想请求共享资源时，需要首先建立三个变量: 准备进入临界区，实时时间和处理器号，并利用广播通信发送给正在运行的所有进程。

该算法的核心思想如下:当进程想进入临界区时,要建立一个包括进入的临界区名字、处理器号和当前时间的消息,并把消息发送给所有其它进程。当进程接收到另一个进程的请求消息时,将分下面三种情况来区别对待:1)若接收者不在临界区中,也不想进入临界区,就向发送者发送OK消息；2)若接收者已经在临界区内就不必回答,而是负责对请求消息排队；3)若接收者要进入临界区,但还没进入,它就会把接收的消息和它发送的消息的时间戳进行对比,取小的那个。如果接收的消息时间戳小,就发OK消息,如果发送的消息时间戳小,那么接收者负责排列请求队列而不发送任何消息。当进程接收到允许消息时,它就进入临界区。从临界区退出时,向队列中的所有进程发送OK消息,并将自己从队列中删除。该算法可以保证访问临界区的互斥性以及无死锁进程、无饥饿进程。

但是这种算法有个严重的缺点是算法太复杂并且不健壮,任何一个进程崩溃都会影响到算法的正确性。

### 令牌环算法

令牌算法中引入了令牌，所有的进程组成一个环模型，环中每个进程需要知道它的下一个位置的节点的名称。令牌在环上顺序传递，当某个进程拥有令牌时就表明可以访问临界区。当请求进程没有令牌时，算法需要 N 发送任何消息。如果得到令牌的进程不打算进入临界区，它只是简单地将令牌传送给它后面的进程。当每个进程都需要进入临界区时，令牌在环上的传递速度最慢; 相反，当没有进程想要进入临界区时，令牌在环上的传递速度最快。

该算法的正确性是显而易见的，但是也存在一些问题,比如说,当令牌丢失时,需要重新生成。可是如何检测令牌丢失又是一个困难的问题。还有,如果环中的一个进程崩溃,那么环的连贯性就遭到破坏,算法也就会出现麻烦。



# 分布式主键

传统数据库软件开发中，主键自动生成技术是基本需求。而各个数据库对于该需求也提供了相应的支持，比如MySQL的自增键，Oracle的自增序列等。

但是随着应用的扩大，我们会考虑数据库的分片的问题，这个时候每个数据结点生产一个全局唯一主键就成了一个非常棘手的问题。
同一个逻辑表内的不同实际表之间的自增键由于无法互相感知而产生重复主键。 

那么分布式主键我们需要考虑那些要求呢：

- 全局唯一性：不能出现重复的ID号，既然是唯一标识，这是最基本的要求。
- 趋势递增：在MySQL InnoDB引擎中使用的是聚集索引，由于多数RDBMS使用B-tree的数据结构来存储索引数据，在主键的选择上面我们应该尽量使用有序的主键保证写入性能。
- 单调递增：保证下一个ID一定大于上一个ID，例如事务版本号、IM增量消息、排序等特殊需求。
- 信息安全：如果ID是连续的，恶意用户的扒取工作就非常容易做了，直接按照顺序下载指定URL即可；如果是订单号就更危险了，竞对可以直接知道我们一天的单量。所以在一些应用场景下，会需要ID无规则、不规则。

其中3和4需求还是互斥的，无法使用同一个方案满足。

同时除了对ID号码自身的要求，业务还对ID号生成系统的可用性要求极高，想象一下，如果ID生成系统瘫痪，整个美团点评支付、优惠券发券、骑手派单等关键动作都无法执行，这就会带来一场灾难。

## UUID

UUID可以在本地生成一个全局的唯一的ID，但是UUID太长，难以作为主键，同时其无序性会导致数据索引B+树的数据位置频繁变动。

## 类snowflake方案

这种方案大致来说是一种以划分命名空间（UUID也算，由于比较常见，所以单独分析）来生成ID的一种算法，这种方案把64-bit分别划分成多段，分开来标示机器、时间等。

其中的高位存储时间，保证了整个ID都是趋势递增的。但是其严格依赖机器的时钟，如果机器上时钟回拨，会导致发号重复或者服务会处于不可用状态。

>Mongdb objectID是一种典型应用，通过“时间+机器码+pid+inc”共12个字节，通过4+3+2+3的方式最终标识成一个24长度的十六进制字符。

## 数据库生成

通过约束不同数据库的自增主键初始值和步长的方式避免碰撞。当具有大量的数据库是，会引入大量额外的运维规则，同时ID没有了单调递增的特性，只是趋势递增。