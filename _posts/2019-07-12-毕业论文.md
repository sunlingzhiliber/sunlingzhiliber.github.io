---
layout:     post
title:      毕业论文
subtitle:   煎熬
date:       2018-07-10
author:     Liber Sun
header-img: img/post-basic.jpg
catalog: true
---

# 现象

Input data and directions
Documentation of all source code, datasets, use cases, and instructions to use WaMDaM and replicate results are available on GitHub and facilitated by Jupyter Notebooks at http://doi.org/10.5281/zenodo.1484581.

Software and data availability
The dataset of academic publications used in this paper is obtained from the Scopus database, and the analysis is implemented in an IPython notebook. Both the dataset and the analysis scripts are available via https://github.com/sibeleker/Validation_Bibliometric.

Acknowledgements
The research was funded by IIASA and its National Member Organizations in Africa, the Americas, Asia, and Europe.

详细的描述使用环境、协议、开发者、联系人

# 观点以及背景

**Integrating scientific cyberinfrastructures to improve reproducibility in computational hydrology: Example for HydroShare and GeoTrust**
作者：Bakinam T. Essawy
可计算环境模型的可再现是一个巨大挑战，他需要开放、可重用的代码、数据，详细的描述的工作流，稳定的环境，以此让别人，让peers能够验证已经发表的结果。
需要原始数据的文档说明，数据预处理脚本，模型输入，模型输出，所有的依赖环境。
提供了模型、数据的共享。
大量的应用，以modelflow的方式来复现可执行的工作流。

GeoTrust Sciunit-CLi提供了可重复计算的技术工作流程，
HydroShare提供了公共存储和元数据支持
pre-processing script 

**GeoJModelBuilder, Wuhan University Using OGC Standards  DOI 10.1186/s40965-017-0022-7**

工作流。


**一种大规模时空数据处理与可视化平台**

目前大多数可视化系统的开发，都是以某一特定的数据集为基础的开发的。
在目前数据集增多以及新的交互设备层出不穷的环境下，会造成开发成本的提升以及开发效率的下降。

终端需求，我们要提供 动态过滤、数据抽取，数据配置。


**Wilkinson, M. D. et al. (2016). The FAIR Guiding Principles for scientific data management and stewardship. Scientific Data, 3, 160018. doi:10.1038/sdata.2016.18**

数据 findable、accesible，interoperable and reusable。
Standards for finding, accessing, analyzing and sharing data, information, models across communities…



**An overview of the model integration process: From pre-integration assessment to testing**

模型继承的步骤：
模型预继承评估 准备模型 模型的编排 数据互操作 测试

Data exchange and data manipulation are fundamental to integrated modeling systems (Argent, 2004; Leimbach and Jaeger,2005),


Ramamoorthy et al. (1992) point out that the main cost of integration is the effort required to detect and resolve inconsistencies in the coupled system, a cost that can be reduced if inconsistencies are detected and resolved prior to integration – that is, during the process of transforming the science design to a software design. 

数据互操作：
 数据的mediation 是由单独的组件模块实现，还是由参与者模型本身来调解数据。
 如果一个模型可以链接5个模型，则它应该包含每个组件的数据集实现。
 为每个模型实现数据集转换函数，当数量增加而缺乏良好的管理性后，这种方法不可扩展。


 Providing models via the web is not without challenges. As Brooking and Hunter (2013) point out, these challenges include managing large volumes of data, complexity of the required software platform, and computational demand for model execution are barriers that limit sharing and re-use of models over the web. Similarly, Nativi et al. (2013) point out developing a flexible architecture to link distributed components, minimizing interoperability agreements, optimizing performance, and securing availability in the long term are challenges in presenting models on the web. Presenting models using web pages also creates a challenge since model usage is constrained by how the web page presents it. For example, users may want to display model output as part of another application, which leads to providing a “model as a service”, making it available as a web service (Geller and Melton, 2008; Geller and Turner, 2007; Roman et al., 2009). This approach makes models and their output more accessible, more comparable, more scalable, and can be implemented using a variety of approaches (Nativi et al., 2013). One example of such a platform is eHabitat (Dubois et al., 2013) which is part of the European Union Digital Observatory for Protected Areas (DOPA20) project. In eHabitat, models and data sources are presented as a pool of OGC WPS services; using a web-based interface, users can select and assemble them like Lego blocks. Another example is the iPlant cyberinfrastructure (Goff et al., 2011) which provides several biology-related applications as a web service API so bioinformatics experts and software developers can embed them in their tools. Some modeling frameworks use purposely-designed semantic technology features to improve discoverability of models and related resources. For example, Galaxy (Goecks et al., 2010) e a computational framework for life sciences e uses tagging (labeling) to describe resources. During simulation, the system automatically tracks input datasets, tools used, parameter values, output datasets, and a series of analysis steps that it stores as history. The user can add annotations about analysis steps. The authors report that tagging supports reproducibility of experiments. Similarly, the system biology data and model platform - SEEK (Wolstencroft et al., 2015) e provides a web-based environment for day-to-day collaboration and public access. It uses the Resource Description Framework (RDF) to store metadata of resources that facilitate data and model exploration. All of these efforts indicate that incorporation of purposely-designed semantic technology features can improve discoverability of models beyond simple search operations.

 可发现性、可访问性和易用性是用户的基本需求。对于现有系统，可发现性通常仅限于组、文献和谷歌搜索中的本地知识。如上所述，文献中常常没有提供足够的细节来确定软件的重要特性，而这些特性是做出重用决策所必需的。值得肯定的是，我们看到了web作为部署建模组件的平台的趋势，这似乎是由于人们希望通过将模型作为服务提供来解决互操作性问题，从而消除不兼容的计算机语言和操作系统的问题。另一个好处是，基于web的部署还增强了模型的可发现性。

**Automatic data matching for geospatial models a new paradigm for geospatial data and models sharing**

网上检索到的数据 缺少语义的支持，往往富含大量无关的冗余数据;丢失大量的隐式相关数据。
就算是模型需要的数据，往往也需要用户去手工检测和识别。

计算f(x) 输入数据和输出数据是否有联系。

自动化转换， 这种转换可能很简单，如空间合并，裁剪。也可能很复杂，涉及到空间升降尺度。

**Probabilistic programming: A review for environmental modellers**

The development process for an environmental model involves multiple iterations of a planning-implementation-assessment cycle.
The needs of environmental modellers differ from those of researchers in many other scientific disciplines because of the common use of both empirical (or black box) and mechanistic models for environmental applications. Consequently, developers and researchers must carefully balance appropriate representations of uncertainty in all its forms with detailed process equations. As the sophistication and computational demands of models continues to grow, issues such as identifiability, reproducibility, and efficient inference will only become more pressing.


**AltEx: An open source web application and toolkit for accessing and exploring altimetry datasets**

基于Tethys Platform(一个github的项目，就是geoserver+可视化+MVC流程)开发的数据获取的APP，你开发就是去写一个请求，写一个静态页面，给一个返回。

不需要专家知识的情况下获取全球任意地区的精确的水位反演，减少获取、处理和可视化数据所需的时间， developing meaningful use cases and applications.


**An interactive web app for retrieval, visualization, and analysis of hydrologic and meteorological time series data**

集成和管理大量的数据流，以产生一致的结果是一项挑战；
每个源都有自己独特的浏览页面、访问方式、权限控制
Furthermore, syntactic and semantic differences in data formats can make it difficult to find, organize, and interpret data (Horsburgh et al., 2009).
Finally, data retrieval and processing can be hindered by the sheer quantity of available data (Vitolo et al., 2015).
集成 数据分析、数据检索和可视化功能。

利用R语言检索数据，提供数据和R带的分析方法

不需要专业代码软件和知识


**A data model to manage data for water resources systems modeling**

Current practices to identify, organize, analyze, and serve data to water resources systems models are typically model and dataset-specific. Data are stored in different formats, described with different vocabularies, and require manual, model-specific, and time-intensive manipulations to find, organize, compare, and then serve to models.


Data analysis and synthesis are fundamental in developing water resources management models (Loucks et al., 2005). Data organization enables or inhibits the analysis that water managers and modelers perform (Brown et al., 2015; Horsburgh et al., 2008). Well organized data can help modelers prepare data for models while poorly organized data can make the process time-consuming and frustrating. Current practices to organize, manipulate, and compare multiple water resources datasets and develop water systems models are typically specific to the data sources, models, and study location (Brown et al., 2015). Source-, model-, and study area-specific practices arise because models have different data requirements for their components, store data in different file formats, have varying spatial and temporal coverage, use inconsistent metadata to describe methods, sources, and units, and use different vocabularies to name similar system components and their attributes (Laituri and Sternlieb, 2014; Maidment, 2016; Miller et al., 2004). These practices limit managers' and modelers’ ability to reuse datasets and models in other applications. To reuse, practitioners often spend up to 75% of their overall modeling time to modify, subset, transform, convert, and restructure data (Beniston et al., 2012; CUAHSI, 2005; Draper et al., 2003; Hey et al., 2009; Leonard and Duffy, 2013; Maidment, 2008; Michener, 2006; Miller et al., 2004; Ridley and Stoker, 2001; Watkins, 2013). **A common database design to organize and manage water resources system data can help modelers and managers spend less time to wrangle with data formats and structures and more effort on analysis to learn about and model systems.**


In current practice, a water resources system modeler selects a water management modeling method and then searches for input data that meets the model's requirements (Brown et al., 2015). Modelers often manually search for, download, synthesize, and compare data from disparate datasets to populate input data (Rosenberg and Madani, 2014). In their data search, modelers often use a combination of existing methods to manually gather input data for the different supply and demand system components and their connectivity from local, state, and federal agencies. Searches can also use national data services like the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI) Water Data Services (Couch et al., 2014; Goodall et al., 2008). **Each dataset has a particular file-format, organizational structure, syntax, and descriptive terminology. Some datasets also come with modeling scenarios that represent changes to values of physical, operational, network topology, or socio-economic attributes of the system. Modelers must reconcile structure and terminology heterogeneities in potential input data.**


The eighth requirement is to develop the WaMDaM implementations using free and open-source software tools, to allow access via an open-source code repository, promote reproducibility, and help others further advance the method (Easterbrook, 2014; Gil et al., 2016; Goodman et al., 2014).

总结了很多数据，生成了一个数据库中的表，将所有的数据放到数据库中，进行查找。





重要：
**Model validation: A bibliometric analysis of the literature**


Validation is a crucial step in environmental and economic modeling that establishes the reliability of models to be used in decision-making contexts. **It is often said that validation approaches proposed in the literature are not widely adopted, and different modeling fields do not benefit from each other. **This study analyses a broad academic literature on model validation, mainly in environmental and decision sciences, by using an innovative combination of bibliometric and text-mining tools. The results show that a data-driven validation practice is prevalent. Although most publications in the studied dataset resemble each other, the most-cited ones tend to be different from the rest in terms of their abstracts’ content. Furthermore, the validation practices in different modeling areas are distinct, and do not extensively cite each other. In future, validation approaches can extend beyond data-oriented reliability for a wider acceptance of modeling in decision-making, and can synthesize the methods and views from various fields.


In line with these critiques that pinpoint data use, model conceptualization, boundaries and assumptions as the most important issues, Smith and Petersen (2014) distinguish between three dimensions of a model's reliability. Statistical reliability refers to the subjective or objective probability distributions communicated in the model-based findings. It covers the concepts of data and behavior (model output) validity. Statistical tests that compare the output of a model to empirical data support this type of reliability. Methodological reliability results from the consideration of model purpose, and it refers to whether the model fits its purpose conceptually and technically. Related to the concepts of conceptual, logical and structural validity, methodological reliability is established by several tests. The commonly used examples of these tests are stress tests (extreme-conditions tests) which check whether the model generates observed or anticipated output when parameters are set to extreme values, or sensitivity analyses which check whether the model outputs are sensitive to its inputs (Balci, 1994; Barlas, 1996). Public reliability indicates the extent of public trust in scientists in general and modelers in particular. This is often proposed to be established by ‘soft’ and participatory approaches (van der Sluijs, 2002).

公共可靠性。


Validation is a crucially important modeling step to establish the reliability of models and expel criticism. In environmental and economic modeling, validation deals mostly with statistical and methodological reliability with several approaches and techniques developed in different areas of environmental science. Whether they focus on model output or structure, these techniques address the representation power of a model, i.e. how well it represents reality. For instance, Matott et al. (2009) present an extensive review of software-based evaluation methods and tools with a focus on statistical reliability, data quality, sampling, input and output uncertainty. Validation approaches in biophysical modeling (Bellocchi et al., 2010), ecological modeling (Augusiak et al., 2014), and environmental modeling (Bennett et al., 2013) acknowledge that validity extends beyond representation, especially beyond an accurate representation of empirical data by model output. Yet, these studies still focus on quantitative, data-oriented techniques that aim to reduce the uncertainty in model outcomes.

定量


参与式评估



它的工作在于： 验证性的出版物的内容关联是什么

模型验证 总是和 **数据**紧密相关 无论是模型输入 还是匹配模型

**预测**，预测导向。

**test**

**compare**

**生态系统和水** 需要做验证 The four main topics found by the topic-modeling algorithm are named as Agriculture, Ecosystems, Hydrology, and Methods, based on their most frequent and most descriptive words. 

**关注参数估计和不确定性。**


A few of the well-known and highly cited publications in the validation literature are marked on Fig. 3, too. Oreskes et al. (1994) state that model validation in a purely positivist way is impossible; therefore, models should be used as heuristics. This article is considerably distant from dense regions of the map, indicating that its rather philosophical content does not have a strong resemblance to most articles. In particular, while Oreskes et al. (1994) contains common words such as predict, evaluate, observe, it also has several uncommon words such as impossible, heuristic and logic. The other two well-known articles (Bennett et al., 2013; Jakeman et al., 2006) address environmental modeling domain specifically and they are positioned relatively close to the central and dense region on the map. Therefore, it can be said that their contents are highly related to the majority of model validation publications in our dataset. In addition to the common words such as data, test, calibrate, these two articles contain the words aim, purpose, tailor, custom frequently, indicating a validation approach based on model purpose, i.e. fit for purpose. Another peripheral article is Schwanitz (2013), which stresses the importance of an integrated validation approach, documentation and communication with stakeholders for transparency, especially for the models used to assess the impacts of climate change on socioeconomic systems, and hence heavily concern public decision-making. Table A1 in the Appendix contains the entire word list of these four articles.


Oreskes et al. (1994) remain highly-cited in Fig. 4b, while the relative citation scores of Jakeman et al. (2006), Bennett et al. (2013) and Schwanitz (2013) increase compared to Fig. 4a.  文章要看



because they discordantly argue that validation based on representation accuracy is impossible. This argument is based on the idea that a match between the model output and observational data does not demonstrate the reliability of a model or hypothesis, it only supports its probability. Therefore, since models can never accurately represent reality, they should not be used for predicting the future but for sensitivity analyses, exploring what-if scenarios, and for challenging our biases and assumptions.

验证输出和观测数据 并不表明模型的可靠性，只是在概率上支持。

这些发现为未来的研究提供了两个主要的建议。数据和预测等词的流行表明，人们非常重视统计和方法的可靠性。公共可靠性是指决策者和利益相关者接受基于模型的结论。因此，未来的研究可以进一步研究如何在广泛的模型验证文献中处理公共可靠性。未来的研究还可以将验证方法从面向数据的可靠性扩展到公共可靠性。其次，由于水文、生态、农业等不同的环境建模领域不仅内容不同，而且交叉引用也不同，未来的研究可以综合各个领域的方法和观点。这样的集成可以增强方法并创建一致的验证实践。

目前的验证实践在确保统计和方法可靠性方面具有很强的优势。因此，未来的研究可以为当前验证实践中如何处理公共可靠性提供更深入的分析。此外，未来的验证研究可以侧重于建立公共可靠性的软方法和参与性方法，以加强在决策上下文中接受和采用基于模型的结论。未来任何环境建模领域的验证研究，如水文、生态系统和能源系统建模，也可以从其他领域的验证方法中受益。综合各个领域的方法、观点和经验，可以加强模型验证实践，以满足未来决策挑战的需求。








**A framework for characterising and evaluating the effectiveness of environmental modelling**


The implementation of the model needs to be reliable in its running and output, and technically valid. Verification benefits from being able to examine the code (e.g. open-source code), as well as from a hierarchical testing approach starting with unit tests, benchmark problems and analytical solutions (NRC, 2012). If not already verified by others, evidence that the algorithms solve the salient mathematical equations should be provided (e.g. NRC, 2012). When executed, the software outputs must adequately capture the conceptual model and reflect descriptions from the model domain. The most current version of the software should be used to reduce potential artefacts from bugs in the software, and solutions should be replicable and reproducible. Bugs are inevitable in all software, including high quality commercial software (McConnell, 2004; Refsgaard and Henriksen, 2004). Therefore in addition to checking inputs and comparing outputs to observed data, internal consistency checks such as assertions and unit testing can be valuable in detecting bugs and ensuring the quality of the model code (Crout et al., 2008; Homès, 2011).

5.2.1. Theoretical basis
Evaluating the theoretical basis of a quantitative model itself involves determining whether concepts, structure and parameterisation schemes are scientifically justified. Because the natural system is unknowably complex, an evaluation of the model basis is to some extent subjective. At a minimum, the conceptual model must form an acceptable approximation of the modelled system. The extent to which the quantitative model incorporates high degrees of conceptual model complexity is driven by the problem being addressed (Jakeman et al., 2006). A theoretical evaluation involves checking all relevant underpinnings used in the model construction, and that model assumptions invoked are justified. Justification may take the form of formal model confirmation (e.g. Refsgaard and Henriksen, 2004), comparison to other models in similar settings, or a check that the model fits client or end-user expectations. When justification is needed, it should support that: 1) the use of the assumptions in the specific circumstances the model is applied; 2) the arguments used are sound; and 3) the assumptions used are not biased toward a modelling outcome. If the theoretical basis of a model is highly uncertain, an associated analysis might instead explore the effect of alternative assumptions, parameterisation schemes, structures or conceptualisations (e.g. Bankes, 1993; Clark et al., 2011).


5.2.2. Credibility of inputs
Adjunct to evaluating the theoretical aspects, those model inputs selected for modelling must also be credible. The inputs should be representative of the drivers of the system, and be suited for the required model scenarios. This does not necessarily mean they directly reflect what is expected in the field – they may capture hypothetical what-if situations. Finally, the inputs should be technically correct, without omissions or errors that are not acknowledged.


5.2.3. Testability of implementation
The implementation of the model needs to be reliable in its running and output, and technically valid. Verification benefits from being able to examine the code (e.g. open-source code), as well as from a hierarchical testing approach starting with unit tests, benchmark problems and analytical solutions (NRC, 2012). If not already verified by others, evidence that the algorithms solve the salient mathematical equations should be provided (e.g. NRC, 2012). When executed, the software outputs must adequately capture the conceptual model and reflect descriptions from the model domain. The most current version of the software should be used to reduce potential artefacts from bugs in the software, and solutions should be replicable and reproducible. Bugs are inevitable in all software, including high quality commercial software (McConnell, 2004; Refsgaard and Henriksen, 2004). Therefore in addition to checking inputs and comparing outputs to observed data, internal consistency checks such as assertions and unit testing can be valuable in detecting bugs and ensuring the quality of the model code (Crout et al., 2008; Homès, 2011).


5.2.4. Match to observed behaviour
Stakeholder acceptance of the model is commonly decided, in part, by its ability to simulate what was observed within the natural system. Typically, a model that approximates observed system behaviour well has higher acceptance than one that does not, though other aspects may be more important depending on the context (Olsson and Andersson, 2006). To increase acceptance of the model for scenario testing, this may also mean that behaviour is evaluated outside of conditions specified for calibration (e.g., for droughts and other extreme conditions) (NRC, 2012, Klemeš, 1986). Formal history matching performance metrics (Bennett et al., 2013) objectively quantify the degree of fit between observations and the model's simulated equivalent outputs. History matching metrics may be difficult to construct in highly complex problems, where data is scarce/unavailable, and where the model is numerically unstable or when the model runtimes are extremely long. Future behaviour is notably fundamentally unknown for example in global climate change models (Schwanitz, 2013). Jakeman et al. (2006) note that comprehensive evaluation of behaviour is “rarely possible (or perhaps even appropriate) for large, integrated models.” Yet even then, stakeholders or experts may be able to identify aspects that are unrealistic, or judge whether differences from observations are tolerable for the modelling purpose. These judgement might be codified as “stylized facts” describing system behaviours that need to be reproduced (Schwanitz, 2013), minimum performance requirements, or fitness for purpose criteria (Haasnoot et al., 2014; Parker, 2009).

5.2.5. Treatment of uncertainty
For model outputs generated in prediction or forecasting mode, assessment of past behaviour typically needs to be complemented by quantification of uncertainty in the actual predictions of interest (Guillaume et al., 2016). Addressing uncertainty requires assessment of variation across many model realisations of possible model inputs for a given structure, and in some cases different model conceptualisations and related structures. That is, one model realisation cannot be considered a full representation of consolidated knowledge (Bankes, 1993; Maier et al., 2016). More broadly, model uncertainty needs to adequately acknowledge alternative paths that could lead to different modelling outcomes (Lahtinen et al., 2017), and adequately (and legitimately) address disagreements as they arise.

What constitutes adequate treatment of uncertainty is highly problem-specific, but it is generally recognised that formal uncertainty evaluation provides indirect benefits such as increasing the depth of analysis (see Guillaume et al., 2017). Uncertainty quantification can provide better understanding of forecast accuracy (see NRC, 2012), which provides for more informed evaluations of risk and reliability. Exploration of sources of uncertainty can be a source of innovation and scientific discovery (Brugnach et al., 2008, p.65). For example, model non-uniqueness can be examined by performing identifiability analysis on hypothetical data that might be collected in the future (Doherty and Hunt, 2009), and help evaluate worth of future data collection (e.g. Fienen et al., 2010). Some model problems benefit from expressing confidence intervals around a prediction/forecast, and there are metrics for measuring quality of uncertainty bounds (Laio and Tamea, 2006; Gneiting et al., 2007; Xiong et al., 2009). Likewise, methods are available to test robustness of management actions (Herman et al., 2015), and a multitude of tools and approaches for working with uncertainty (Refsgaard et al., 2007; Matott et al., 2009; Van Der Sluijs et al., 2005; Jakeman et al., 2006). In specific domains, there may be guidelines or stated protocols for evaluating the treatment of uncertainty, but typically each modelling project will have its own criteria for determining whether adequate consideration is given to whether alternative paths might have resulted in different outcomes.


5.3.2. End-user input
End-user engagement is often considered critical for ensuring relevance of model results. The USA National Research Council asserted that “inadequate progress has been made in synthesizing research results, assessing impacts of climate change on human systems, or providing knowledge to support decision making and risk analysis” (NRC, 2007). Reasons included a lack of meaningful interaction between scientists and decision makers, and difficulty in interpreting scientific information and results and translating them into recommendations for action. Dunn and Laing (2017) came to similar conclusions about the disconnection between research and policy more recently. Rather than assuming stakeholder needs are known, it is useful to have users at the discussion table to frame the problem initially, or establishing two-way, iterative engagement between producers and users to build trust and better understand the needs of policy-makers (Dilling and Lemos, 2011).

Furthermore end-user engagement is important from a social learning perspective, which is the idea that people learn in groups; this has emerged as imperative for mediating the science-policy divide (Gober, 2018; Glynn et al., 2017). Managing environmental systems in an era of uncertainty requires the capacity to learn from experience, synthesise different types of knowledge and experiences, and view policies as learning experiments (Folke et al., 2005). From a water resource perspective, this means greater interaction and mutual learning from scientists and water managers. The need for social learning implies new roles for scientists in the water management process, moving from providers of scientific tools and insights to partners in the use of tools and insights for policy and adaptation decision-making (Pahl-Wostl, 2009; Pahl-Wostl et al., 2010).

Direct end-user engagement may be beneficial, including co-authorship between scientists and decision makers. However, it is not always appropriate for modellers to engage directly with users. Instead, engagement might be structured through knowledge brokers or boundary organisations who can negotiate tensions and facilitate useful exchange between scientists and decision makers, linking science and decision-making, and building collaboration and cooperation (White et al., 2008; Crona and Parker, 2012). Boundary agents help ensure that scientists provide information that fits the given policy context. Boundary objects such as water resource models are an obvious way for the two groups to work together, but uneven power relations and the institutional differences between academic and public sector employment can stymie their role in mediating interests of the two groups (White et al., 2010).

5.3.3. Timeliness
Even where modelling is in principle relevant and end users are appropriately engaged, the timeliness of scientific activities needs to be right (Cash et al., 2003). If modelling exercises are run or model results are published at an inconvenient or inopportune time (practically or politically), then they may be less likely to be salient or have an impact. In a policy context, timing needs to fit in the policy decision window when the need for change is widely acknowledged and participants feel they can make a difference (Huitema and Meijerink, 2010). This is not to say that modelling should not proceed outside those times. Indeed, often modelling exercises need to have occurred prior to the decision window in order for model results to be available. Rather, the purpose and design of the project and its evaluation should be adjusted to reflect these timing constraints. For example, a modelling exercise that loses its salience due to political events can still be effective if it instead aims to ready materials for the next decision window – and the evaluation needs to reflect this new aim.


5.4. Legitimacy
Legitimacy is the extent to which decision makers or stakeholders feel that the science or model was developed and presented in an open and unbiased way, respectful of divergent points of view in the community (Cash et al., 2003). This criteria includes the acceptance of the authority of the modelling process to influence decision making (based on Lockwood et al., 2010). Legitimacy is closely related to notions of fairness and justice (Syme et al., 1999) It is critical for all models, particularly when there is a human dimension involved, which is the case for any model focussed on decision support. Without legitimacy of the model, subsequent scientific advice and decisions may also not be perceived as legitimate. Legitimacy may often be observed retrospectively, through increasing commitment of decision makers, from exploratory conversations and brokering the pros and cons of potential decisions to later events that mobilise action on agreed upon policy. We focus here on early indicators: inclusion, lack of bias and trust.

5.5.1. Input and output accessibility, and model transparency and traceability
The utility of the model strongly depends on its accessibility in terms of the usability of the model and its outputs and how well they are understood. We consider three criteria relevant to model accessibility in the immediate to short term and another three criteria, described later, relevant in the medium to long term. The immediate to short term criteria are: input accessibility, which relates to the ease of use of the model by the intended end user to perform the task for which it was designed, including the effort required to preprocess data as model input; output accessibility which relates to whether the model results can be understood, again to the intended audience; and model transparency which refers to whether the inner workings of the model are available to users. Model transparency includes the accessibility of the theory and assumptions underpinning the model to enable a deeper interpretation of the model results. Comprehensive documentation of the rationale of the model, its development process, the intended area of application and its limitations can reduce uncertainty about how the model can be applied (Crout et al., 2008).

Key to these three short-term accessibility criteria is ensuring the model and its outputs are suited to the target audience, whether they be decision makers, scientists or community members. These criteria are relevant to all models, regardless of the project context. The model should be designed to bridge the gap between the technological aspects of the model and the cognitive aspects of end users, shaped by their background and technical levels. This bridging can be achieved, for example, through the design of a user-friendly software interface (GUI) and provision of a non-technical (e.g. written in plain English) user manual for operating the model and interpreting results (McIntosh et al., 2011). If the system is non-intuitive to the end user and difficult to navigate, long-term adoption (especially with staff turnover) may not be achieved even if training is provided in the beginning. Others have also found that the complexity of models may contribute to model rejection (Kolkman et al., 2016).

On the other hand, it has also been argued that a user interface that is too simplistic can reduce the transparency of the analyses occurring within environmental models, undermining the user's satisfaction that the complexity of the problem has been adequately captured (Matthews et al., 2011; Stirling, 2010). This suggests the importance of matching the model's degree of complexity and transparency with the user's capacity and expectations (Gilbert et al., 2018). For some users to trust the model, it may be important that it not be a black box and users are able to access and trace the logic of its complex inner workings; this may be through documentation.

Ideally, model accessibility should be tested by end users continuously throughout the development process and before the model is delivered (i.e. formative evaluation; user acceptance testing) to allow modifications to the design of the model to better suit end user requirements (Otaduy and Diaz, 2017). These accessibility criteria may not be perceived as important in many evaluation contexts in comparison to more outcome based (e.g. system level) criteria. However, model accessibility is critical to its use and may be the underlying factor determining whether or not a model is actually used and has subsequent impact.

5.5.2. Reusability, flexibility and ease of maintenance
The three shorter term criteria discussed above (Section 5.5.1) then interplay in the medium to long term through influencing model reusability, flexibility, and ease of maintenance. The importance of these additional criteria varies depending on the purpose of the model. Reusability refers to both i) running and re-running the model, and ii) the action of repurposing the model for other applications and contexts. Flexibility on the other hand refers to the ease with which modifications can be made to include additional processes or exclude less relevant processes to better fit a model for its purpose. Ease of maintenance is defined as an attribute of the model that enables defects or issues to be identified and resolved with minimal effort on the part of the model maintainer. This can be achieved through the use of software testing principals, including the creation and maintenance of a test suite, which aids in alerting the maintainer when a change to the model causes adverse effects or has unintended consequences (Crout et al., 2008; Homès, 2011).

Model reusability hinges on the technical implementation details of the model (how it was developed) and how well its use is documented (Holzworth et al., 2010). A model cannot be considered reusable if it cannot be applied to a similar but new context without significant modification to the underlying code. Similarly, insufficient user documentation hinders model reusability as the model cannot be reapplied to a new context without the user having prior knowledge of how to do so – an example of model transparency affecting reusability.

Flexibility plays an important role in the medium term as poor model flexibility negatively impacts development velocity – the speed at which improvements and modifications can be made. An inflexible model structure hinders the ability to incorporate new information, knowledge, and data, such as those that may come to light through an iterative development process (Krause and Flügel, 2005; Formetta et al., 2014). In the longer term, lack of flexibility compromises model reusability as relevant processes may not be adequately captured for the model's intended purpose.

It is not advocated here that all models be made reusable or flexible, and expending considerable energy on ensuring ease of maintenance may not be appropriate. These all depend on the given modelling purpose. However, the benefits of models that may be repurposed and adjusted for different contexts is increasingly acknowledged by the environmental modelling community (de Kok et al., 2015). Approaching model development in this manner increases a model's flexibility of use. Repurposing a research tool for use in participatory or decision support contexts is better achieved if code and processes are documented and changes to the code base do not adversely and unnecessarily propagate throughout the model structure. As a beneficial side-effect, such ancillary support processes increase a model's ease of maintainability. However, adopting development approaches to support reusability and flexibility is often a secondary concern (de Kok et al., 2015; Hutton et al., 2016).

These accessibility criteria may not be a factor in cases where use of the model beyond its initial purpose is not intended. This may be in cases where a single context-specific model is agreed to by end users prior to the delivery of the model. It is notoriously difficult and costly, however, to introduce reusability, flexibility, and ease of maintenance after the fact. Such difficulties are well documented in both domains of software and model development, giving rise to iterative development practices (as evidenced by Jakeman et al., 2006; Larman and Basili, 2003). Where in line with the purpose of model development, the criteria of model reusability, flexibility, and ease of maintenance should ideally be considered from the beginning of the development process.


5.7. Satisfaction
The success of a model can also be gauged by the appraisal of the end product and modelling process by the different groups of people associated with the project or the modelled problem, including the end-user or client who funded the project, the project team, stakeholders or independent reviewers. In many ways, project satisfaction is an aggregate measure of all criteria perceived as important by the individual or group. As with any summative judgement, this assessment will be influenced by the respondent's personal attributes (e.g. values, beliefs, personal norms, knowledge and skills) as well as their experience with the model, including their level of engagement with the process, and their understanding and expectations of the model and/or modelling process (Olsson and Andersson, 2006; Hunka et al., 2013).

The purpose of the model as well as the purpose of the evaluation will determine the value of the respective satisfaction criteria, i.e. whose satisfaction is important? For instance, for a research tool, independent satisfaction (e.g. expert peer review) may be most important. For a decision support tool for operational management, end-user satisfaction is fundamental, but a decision support tool for management of more controversial issues, such as water allocation, may require satisfaction by stakeholders such as the affected communities. On the other hand, stakeholders engaged with the participatory modelling process may provide valuable feedback on the social learning achievements of the project. The satisfaction of the project team may be useful for evaluating the value of methodological practices, but may be considered too biased for assessing the overall merits of the work.

Satisfaction can be assessed at or after completion of the project. If this appraisal is undertaken shortly after the project is delivered, the detailed aspects of the project are more likely to be recalled. However, many outcomes, particularly system-level impacts, may yet be realised. On the other hand, satisfaction appraisals conducted many years after project completion may be able to capture more types of outcomes, as well as information on long-term model usage or changes. However, as time goes on it may be more difficult to engage with the relevant people, for example staff members originally engaged with the project may have left the organisation.

**Sharing and performance optimization of reproducible workflows in the cloud**

科学工作流在现代科学中扮演着重要的角色，因为它使科学家能够指定、共享和重用计算实验。为了使效益最大化，工作流需要支持它们所捕获的实验方法的重现性。由于科学家可以重新执行他人开发的实验，并迅速得出新的或改进的结果，可重复性使有效共享成为可能。然而，在实践中实现重现性是有问题的——以前的分析强调了由于输入数据、配置参数、工作流描述和用于实现工作流任务的软件的不受控制的更改而导致的问题。由此产生的问题被称为工作流衰减。

Recent research on workflows and workflow management systems has found that a large number of workflows cannot be reused, nor can they produce the same results over time [5]. This has been termed workflow decay [6] and stems from a variety of factors, including: the lack of an adequate workflow description, missing resources required to execute workflows such as data and services, and changes in the workflow execution environment [7].

我们的科学工作流程可重复性框架的完整描述，包括:工作流程建模、动态部署、图像管理和版本控制，

新的性能优化技术，增强我们的重现性框架，包括:
一种命名、创建和选择兼容任务图像的新算法，提高了准备运行工作流组件的可重用性，
支持工作流共享和优化工作流部署过程的可部署组件的多级缓存，
任务工件和依赖包的缓存，以支持创建映像的过程，
使用在本地和云环境上运行的真实和合成的科学工作流来验证和评估所提议的机制的一组实验。这些显示了工作流供应和制定的性能改进，以及工作流共享方面的改进

镜像~~~~


# 仔细研读的论文

A framework for characterising and evaluating the effectiveness of environmental modelling

Model validation: A bibliometric analysis of the literature

Sharing and performance optimization of reproducible workflows in the cloud